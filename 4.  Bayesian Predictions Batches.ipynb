{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c385c605-f254-47c8-8726-6db0b69b40b2",
   "metadata": {},
   "source": [
    "# Bayesian MBG Predictions (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6cd79a-32fd-4894-8dd7-c583e8a11e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions\n",
    "\n",
    "import pymc as pm\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytensor.tensor as at\n",
    "\n",
    "import uuid\n",
    "import pytensor.tensor as at\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89a092",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Geostatistical Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9b3df-09bf-413b-83ef-368d2a7019a1",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b6f9ef-83a4-4f23-8fdc-8c771b43fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load report\n",
    "with open('temp_files/report/report.pkl', 'rb') as pickle_file:\n",
    "    report = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c344177-fc16-4d28-98ad-ca9511cfb782",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    }
   ],
   "source": [
    "#Define the directory where the pickle files are stored\n",
    "pickle_dir = 'temp_files'\n",
    "\n",
    "target_indicator = report['Target Indicator']\n",
    "\n",
    "# Get the number of CPU cores to max out the machine in the traning stage\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9113a7-fc0d-4394-8a6c-0772770197d1",
   "metadata": {},
   "source": [
    "### Load target and covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "409f213f-8414-4f03-8be9-2afee28fdbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = pd.read_pickle('temp_files/selected_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea1c0f3-3d32-47b2-81f8-02c388f71d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = gdf.columns.to_list()\n",
    "\n",
    "# Remove target_values and others\n",
    "remove_list = [target_indicator, 'geometry', 'grid_id']\n",
    "\n",
    "# Remove elements in remove_list from main_list\n",
    "selected_features = [item for item in selected_features if item not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09181019-9490-4cf4-ba8c-fbe4979d8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from the geometry column (using centroid for Polygons)\n",
    "coordinates = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf.geometry])\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for observed rows\n",
    "coordinates_observed = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[~gdf[target_indicator].isnull()].geometry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8129a75e-3878-47ef-8df9-7d83c3cfd474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only rows with observed target indicator\n",
    "df1 = gdf[~gdf[target_indicator].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a059c9ad-3b9c-400c-b02d-96175ffced8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness: [0.63607374], Kurtosis: [-0.1470957]\n",
      "Applying square root transformation due to moderate positive skewness and non-positive values.\n"
     ]
    }
   ],
   "source": [
    "# Select and apply the best transformation\n",
    "target_transformed, transform, lmda = functions.select_transformation(df1[[target_indicator]])\n",
    "\n",
    "#Store important information for reversion\n",
    "\n",
    "report['Transformation Applied'] = transform #Transformation applied to target\n",
    "report['Lambda'] = lmda #Store lambda variable for some reversion processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7fe8986-8b1e-4c74-be46-5243f0988069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Daniel/miniconda3/envs/pymc_env/lib/python3.12/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "#Replace target for target_transformed in the df\n",
    "df1[target_indicator] = target_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d735d4c-eef8-4102-8a97-83f491d050ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformed target variable\n",
    "y = df1[target_indicator].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d70c106-0aab-4bb8-9166-d16cf1efff9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariate matrix\n",
    "X = df1[selected_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0be0385-869b-4dc9-9bb0-400007dbd2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize features and transformed y\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "y = (y - y.mean()) / y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6692a5-2a5c-4d64-bd1a-6de75ffcb31c",
   "metadata": {},
   "source": [
    "### Recreate the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c77e4275-d0d3-4ff6-bf02-3a818ea08a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After fitting the model\n",
    "trace_filename = 'model_trace.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ba34396-fef0-40cb-bf7e-4e4bed7b9aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the model\n",
    "idata = az.from_netcdf(trace_filename)\n",
    "\n",
    "# Re-create the model\n",
    "with pm.Model() as model:\n",
    "    # Re-create the model's priors and components\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    ls = pm.HalfCauchy('ls', beta=1)\n",
    "\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c0504-c686-496f-90fc-8810d74a483a",
   "metadata": {},
   "source": [
    "## Generating predictions for all grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "518e121b-3c23-4ec9-96d8-d1a336f3a950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = gdf[gdf[target_indicator].isnull()]\n",
    "\n",
    "#df2 = df2.sample(400) #temporary to make some tests\n",
    "\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Covariate matrix\n",
    "X_new = df2[selected_features].values\n",
    "\n",
    "# Standardize the new data using the same scaler fitted on the observed data\n",
    "X_new = (X_new - X_new.mean(axis=0)) / X_new.std(axis=0)\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for unobserved rows\n",
    "coordinates_new = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[gdf[target_indicator].isnull()].geometry])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83bd58-f900-4554-875a-9bc99ec6f17b",
   "metadata": {},
   "source": [
    "### Check if the covariance matrix is PSD\n",
    "- PSD: Positive Semi-Definitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7ae0015-27f1-47b4-9cce-fd6406b70533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function is key to diagnose what is going on inside the Gaussian process\n",
    "\n",
    "def diagnose_covariance_matrix(cov, jitter=1e-6):\n",
    "    \"\"\"\n",
    "    Diagnose potential issues with a covariance matrix and suggest possible remedies.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    cov : np.ndarray or pytensor.tensor\n",
    "        The covariance matrix to diagnose.\n",
    "    jitter : float, optional\n",
    "        The amount of jitter to add to the diagonal of the covariance matrix for stabilization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert pytensor tensor to numpy array for diagnosis if necessary\n",
    "    if isinstance(cov, at.TensorVariable):\n",
    "        # Use pm.draw to evaluate the tensor as a NumPy array\n",
    "        cov = pm.draw(cov)\n",
    "\n",
    "    # Check for symmetry\n",
    "    if not np.allclose(cov, cov.T):\n",
    "        print(\"Warning: Covariance matrix is not symmetric.\")\n",
    "    else:\n",
    "        print(\"Covariance matrix is symmetric.\")\n",
    "\n",
    "    # Check for positive semi-definiteness using eigenvalues\n",
    "    eigvals = np.linalg.eigvalsh(cov)\n",
    "    if np.all(eigvals >= 0):\n",
    "        print(\"Covariance matrix is positive semi-definite (PSD).\")\n",
    "    elif np.all(eigvals > 0):\n",
    "        print(\"Covariance matrix is positive definite (PD).\")\n",
    "    else:\n",
    "        print(\"Covariance matrix is not positive semi-definite (non-PSD).\")\n",
    "        print(\"Eigenvalues:\")\n",
    "        print(eigvals)\n",
    "\n",
    "    # Check for small or negative eigenvalues\n",
    "    if np.any(eigvals < 0):\n",
    "        print(\"There are negative eigenvalues, indicating non-PSD matrix.\")\n",
    "    elif np.any(eigvals == 0):\n",
    "        print(\"There are zero eigenvalues, indicating the matrix is singular or nearly singular.\")\n",
    "    if np.any(eigvals < jitter):\n",
    "        print(\"Some eigenvalues are smaller than the jitter value. Consider increasing jitter.\")\n",
    "\n",
    "    # Check the condition number (ratio of max to min eigenvalue)\n",
    "    cond_number = np.linalg.cond(cov)\n",
    "    print(f\"Condition number of the matrix: {cond_number:.2e}\")\n",
    "    if cond_number > 1e10:\n",
    "        print(\"Warning: Covariance matrix is ill-conditioned (large condition number).\")\n",
    "        print(\"Consider regularization or using a different covariance function.\")\n",
    "\n",
    "    # Suggest adding jitter and re-check PSD\n",
    "    cov_with_jitter = cov + jitter * np.eye(cov.shape[0])\n",
    "    eigvals_with_jitter = np.linalg.eigvalsh(cov_with_jitter)\n",
    "    if np.all(eigvals_with_jitter >= 0):\n",
    "        print(\"Adding jitter made the covariance matrix positive semi-definite.\")\n",
    "    else:\n",
    "        print(\"Even after adding jitter, the matrix is still not positive semi-definite.\")\n",
    "\n",
    "    # Check for numerical issues using Cholesky decomposition\n",
    "    try:\n",
    "        np.linalg.cholesky(cov)\n",
    "        print(\"Cholesky decomposition succeeded: Covariance matrix is positive definite.\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Cholesky decomposition failed: Covariance matrix is not positive definite.\")\n",
    "\n",
    "    print(\"\\nDiagnosis Complete.\")\n",
    "\n",
    "def generate_predictions(model, coordinates_new, X_new, idata, initial_jitter=1e-6, max_attempts=5):\n",
    "    \"\"\"\n",
    "    Generate predictions for new data using a Gaussian Process model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : pm.Model\n",
    "        The PyMC model object that contains the Gaussian Process.\n",
    "    coordinates_new : np.ndarray\n",
    "        An array of coordinates for the new data points where predictions are needed.\n",
    "    X_new : np.ndarray\n",
    "        The covariate matrix for the new data points.\n",
    "    idata : az.InferenceData\n",
    "        The InferenceData object containing posterior samples from the fitted model.\n",
    "    initial_jitter : float, optional\n",
    "        The initial jitter value to add to the covariance matrix to ensure positive definiteness.\n",
    "    max_attempts : int, optional\n",
    "        Maximum number of attempts to find a stable jitter value.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of mean predictions for the new data points.\n",
    "    \"\"\"\n",
    "\n",
    "    with model:\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                jitter = initial_jitter * (10 ** attempt)\n",
    "                unique_name = \"f_pred_\" + str(uuid.uuid4())\n",
    "\n",
    "                # Generate the conditional GP with added jitter to the covariance matrix\n",
    "                f_pred = gp.conditional(unique_name, coordinates_new, jitter=jitter)\n",
    "\n",
    "                # Compute the mean of the beta samples\n",
    "                beta_mean = idata.posterior['beta'].mean(dim=(\"chain\", \"draw\")).values\n",
    "\n",
    "                # Predictive mean\n",
    "                mu_pred = pm.math.dot(X_new, beta_mean) + f_pred\n",
    "\n",
    "                # Create the covariance matrix using PyMC's Matern32\n",
    "                cov = pm.gp.cov.Matern32(coordinates_new.shape[1], ls=1.0)(coordinates_new)\n",
    "\n",
    "                # Add jitter using Pytensor's identity matrix\n",
    "                cov += jitter * at.eye(cov.shape[0])\n",
    "\n",
    "                # Symmetrize the covariance matrix to ensure symmetry\n",
    "                cov = (cov + cov.T) / 2\n",
    "\n",
    "                # Check cov_matrix before predictions\n",
    "                diagnose_covariance_matrix(cov)\n",
    "                \n",
    "                # Check for positive definiteness using Cholesky decomposition\n",
    "                _ = at.slinalg.cholesky(cov)\n",
    "\n",
    "                # If successful, proceed with prediction\n",
    "                pred_samples = pm.sample_posterior_predictive(idata, var_names=[unique_name], return_inferencedata=True)\n",
    "                return pred_samples.posterior_predictive[unique_name].mean(axis=0)\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == max_attempts - 1:\n",
    "                    raise ValueError(f\"The covariance matrix is not positive semi-definite even after {max_attempts} attempts with increasing jitter.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9126074-9c86-4dc6-a08f-ba65477a0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe to store predictions and other values for uncertainty calculations\n",
    "df3 = pd.DataFrame(df2[['grid_id', 'mpi']].head(0))\n",
    "begin_from_scratch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c624d4e-84c9-454d-9413-50ae25beb641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick-up productions were we left of. \n",
    "with open('temp_files/predictions.pkl', 'rb') as pickle_file:\n",
    "    df3 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be0f92b1-3087-4c24-8aaa-c6febe949389",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position = df3.index.max() + 1\n",
    "\n",
    "#Zero if df3 has been just initialized\n",
    "if begin_from_scratch == True:\n",
    "    start_position = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2244d097-fd5b-46ab-81a2-c150eeb66daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10396f41-a434-4b0a-b174-4b69ebd9162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 95% confidence level, change alpha for different confidence levels\n",
    "alpha = 0.05\n",
    "z_score = stats.norm.ppf(1 - alpha/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e503963-25e4-4c67-ac9b-900d82842177",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98c403-4f27-4c08-ba1d-443e26691317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 100)\n",
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 1.96e+06\n",
      "Adding jitter made the covariance matrix positive semi-definite.\n",
      "Cholesky decomposition succeeded: Covariance matrix is positive definite.\n",
      "\n",
      "Diagnosis Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_c813c011-427d-4b10-b134-cd791ad303ad]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eb3aaee9f84d78aa0e134a7a8e1100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 1.48e+06\n",
      "Adding jitter made the covariance matrix positive semi-definite.\n",
      "Cholesky decomposition succeeded: Covariance matrix is positive definite.\n",
      "\n",
      "Diagnosis Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_3b30daa5-abf4-45f4-9939-e5c965e85b98]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad15c909e6e40db81873e1d8b9d6a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 4.35e+05\n",
      "Adding jitter made the covariance matrix positive semi-definite.\n",
      "Cholesky decomposition succeeded: Covariance matrix is positive definite.\n",
      "\n",
      "Diagnosis Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_41e55111-f828-4f24-b0d5-c43e0d4f1c9d]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17d7f50f1b140de9bf9c6c7217fc4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 5.39e+04\n",
      "Adding jitter made the covariance matrix positive semi-definite.\n",
      "Cholesky decomposition succeeded: Covariance matrix is positive definite.\n",
      "\n",
      "Diagnosis Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_4c8b28ff-71da-46cc-a58b-f93dc582e44e]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e99836e5f884c4d934c7f486e8b3063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(start_position, len(df2), step):\n",
    "\n",
    "    r = range(i, i+step)\n",
    "\n",
    "    print(r)\n",
    "    \n",
    "    X_new_i = X_new[r]\n",
    "    \n",
    "    coordinates_new_i = coordinates_new[r]\n",
    "    \n",
    "    predictions_i = generate_predictions(model, coordinates_new_i, X_new_i, idata)\n",
    "    \n",
    "    # Extracting relevant data\n",
    "    dfi = df2.loc[r][['grid_id', target_indicator]]\n",
    "    dfpi = pd.DataFrame(predictions_i)\n",
    "    \n",
    "    # Calculating mean, standard deviation, and other statistics\n",
    " \n",
    "    dfi[target_indicator] = dfpi.mean().values  # Mean prediction for each observation\n",
    "    dfi['std'] = dfpi.std().values              # Standard deviation for each observation\n",
    "    dfi['max'] = dfpi.max().values              # Max prediction for each observation\n",
    "    dfi['min'] = dfpi.min().values              # Min prediction for each observation\n",
    "     \n",
    "    # Calculating the margin of error for the confidence interval\n",
    "    dfi['standard_error'] = dfi['std'] / (len(dfpi)**0.5)\n",
    "    dfi['margin_of_error'] = z_score * dfi['standard_error']\n",
    "    \n",
    "    # Calculating the confidence interval\n",
    "    dfi['ci_lower'] = dfi[target_indicator] - dfi['margin_of_error']\n",
    "    dfi['ci_upper'] = dfi[target_indicator] + dfi['margin_of_error']\n",
    "    \n",
    "    df3 = pd.concat([df3, dfi])\n",
    "\n",
    "    #Save predictions so far\n",
    "    df3.to_pickle('temp_files/predictions.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pymc_env",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
