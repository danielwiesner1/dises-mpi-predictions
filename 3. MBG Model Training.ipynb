{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400cf36e-fd75-4aa5-ab3c-435b61d429f6",
   "metadata": {},
   "source": [
    "# Bayesian MBG estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1a93-6dcf-4650-b132-0804b7ea15bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Geostatistical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76b10a-d4d2-46e3-82c9-33f84a74183d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "#import aesara.tensor as at\n",
    "#from aesara.graph.basic import Constant\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytensor.tensor as at\n",
    "import functions\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963da62-f8d4-45f8-9b99-27a5d861889d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of CPU cores to max out the machine in the traning stage\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfb7c0-e567-4932-a955-80bc250cc348",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd783437-e4a1-4e88-bcff-e3450d023b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load report\n",
    "with open('temp_files/report/report.pkl', 'rb') as pickle_file:\n",
    "    report = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a32c0d-bd75-4d8f-8e83-db97faf5c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7895e6-677b-4691-b151-d288cc4d333e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_indicator = report['Target Indicator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c6412-da59-4b27-982a-ca71851c01ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the directory where the pickle files are stored\n",
    "pickle_dir = 'temp_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf4655-7c75-4007-a06e-f57d8d3c1106",
   "metadata": {},
   "source": [
    "### Load the target and the covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee20c5a-cbaa-447c-ada0-e48a2cfa59d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = pd.read_pickle('temp_files/selected_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09181019-9490-4cf4-ba8c-fbe4979d8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from the geometry column (using centroid for Polygons)\n",
    "coordinates = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf.geometry])\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for observed rows\n",
    "coordinates_observed = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[~gdf[target_indicator].isnull()].geometry])\n",
    "\n",
    "# Extract coordinates from the geometry column (using centroid for Polygons) only for unobserved rows\n",
    "coordinates_unobserved = np.array([(geom.centroid.x, geom.centroid.y) for geom in gdf[gdf[target_indicator].isnull()].geometry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319008a-3927-4d40-83bf-8ee13c4f7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop geometry\n",
    "df = gdf.drop('geometry', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad7694-6275-464b-bbea-dddf156a2875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only rows with observed target indicator\n",
    "df1 = df[~df[target_indicator].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a6a622-e2f3-40e9-8f05-da370313c84f",
   "metadata": {},
   "source": [
    "### Transform the target indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d1253-a6e8-45a4-8444-d7805a68cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select and apply the best transformation\n",
    "target_transformed, transform, lmda = functions.select_transformation(y = df1[[target_indicator]])\n",
    "\n",
    "## Store important information for reversion\n",
    "\n",
    "report['Transformation Applied'] = transform #Transformation applied to target\n",
    "report['Lambda'] = lmda #Store lambda variable for some reversion processes\n",
    "\n",
    "## Replace target for target_transformed in the df\n",
    "df1[target_indicator] = target_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6189a-79f0-4873-a516-9146b12464df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = df1.columns.to_list()\n",
    "\n",
    "# Remove target_values and others\n",
    "remove_list = [target_indicator, 'geometry', 'grid_id']\n",
    "\n",
    "# Remove elements in remove_list from main_list\n",
    "selected_features = [item for item in selected_features if item not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dee749-e44c-41bd-9673-7361cd837b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariate matrix\n",
    "X = df1[selected_features].values\n",
    "\n",
    "# Series with the target variable observed\n",
    "response = df1[target_indicator].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d988c-e248-4632-9c0d-a41be57a4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the mean and the std for inverse transformation\n",
    "report['Target mean'] = response.mean()\n",
    "report['Target std'] = response.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95225ff3-20cb-425d-b022-a318447b4ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize features and response\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "response = (response - response.mean()) / response.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51698eae-69a6-4984-be01-62ab9d92868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarize the coordinates\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the coordinates\n",
    "coordinates_observed = scaler.fit_transform(coordinates_observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bee51-f03b-413a-bdc8-1d0450100073",
   "metadata": {},
   "source": [
    "### Bayesian Model Training\n",
    "\n",
    "Key Components of the Model\n",
    "\n",
    "    Priors:\n",
    "        beta: Coefficients for the linear model, assumed to follow a normal distribution with mean 0 and standard deviation 1.\n",
    "        sigma: Standard deviation of the observation noise, assumed to follow a half-normal distribution with standard deviation 1.\n",
    "        ls: Length-scale parameter for the spatial covariance function, assumed to follow a half-Cauchy distribution with scale parameter 1.\n",
    "\n",
    "    Spatial Distance Matrix:\n",
    "        D: Matrix of Euclidean distances between all pairs of observed locations.\n",
    "\n",
    "    Covariance Function:\n",
    "        K: Covariance function (Matern 5/2) which defines the spatial correlation structure.\n",
    "\n",
    "    Gaussian Process (GP):\n",
    "        gp: Latent Gaussian process with the defined covariance function.\n",
    "        f: Prior distribution of the GP evaluated at the observed coordinates.\n",
    "\n",
    "    Linear Model:\n",
    "        mu: Mean of the linear model which is a combination of the linear predictor (X * beta) and the spatial effect (f).\n",
    "        y_obs: Observed responses, modeled as a normal distribution with mean mu and standard deviation sigma.\n",
    "\n",
    "    Inference:\n",
    "        Using Automatic Differentiation Variational Inference (ADVI) to approximate the posterior distribution of the model parameters.\n",
    "        advi_fit: Fitting the model using ADVI.\n",
    "        trace: Sampling from the fitted model to obtain posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa606405-d245-4987-a9e2-837650ae2bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit a Bayesian geostatistical model\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    ls = pm.HalfCauchy('ls', beta=1)\n",
    "\n",
    "    print('Priors run')\n",
    "\n",
    "    # Spatial distance matrix\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    print('Distance matrix calculated')\n",
    "\n",
    "    # Define covariance function\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    print('Covariance run')\n",
    "\n",
    "    # Linear model\n",
    "    ## This defines the mean of the normal distribution for the observed data. It combines a linear regression term (pm.math.dot(X, beta)) with the GP latent function f.\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "\n",
    "    ## This defines the likelihood of the observed data (response) as a normal distribution with mean mu and standard deviation sigma.\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=response)\n",
    "\n",
    "    print('Linear model specified')\n",
    "\n",
    "    # Inference\n",
    "    step = pm.NUTS(target_accept=0.95)\n",
    "    idata = pm.sample(1000, tune=1000, step=step, cores=num_cores, return_inferencedata=True) #The num_cores parameter maxes the machine out. Tweak if needed. \n",
    "\n",
    "    print('Model Fitted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b14e9-5098-4917-9afd-8fe2ebc7ffa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This part saves the trained model in case is needed\n",
    "# Save the InferenceData (trace)\n",
    "trace_filename = 'model_trace.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f73a49-d761-45dd-8455-09767a68fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.to_netcdf(idata, trace_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007bc75-b92e-492a-bd8b-e4e67a31b734",
   "metadata": {},
   "source": [
    "### Reload the model\n",
    "- In case of crash load this insted of the fitting cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7141063-436e-4748-9423-67075141ed99",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Load the model\n",
    "idata = az.from_netcdf(trace_filename)\n",
    "\n",
    "# Re-create the model\n",
    "with pm.Model() as model:\n",
    "    # Re-create the model's priors and components\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    ls = pm.HalfCauchy('ls', beta=1)\n",
    "\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b3166-cca2-409b-8bcc-ee678ac1673f",
   "metadata": {},
   "source": [
    "## Testing the model results\n",
    "\n",
    "1. Posterior Predictive Checks\n",
    "2. Prediction Accuracy Metrics\n",
    "3. Residual Analysis\n",
    "4. Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d960452-f2b8-480e-8571-17dd1e06afe0",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks:\n",
    "\n",
    "Posterior Predictive Distribution: Compare the observed data to the posterior predictive distribution of the model. This involves generating new data based on the posterior distributions of the model parameters and comparing these simulated data to the actual observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3707ef-231e-443a-9bb1-13368730d3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Plot posterior predictive checks\n",
    "az.plot_ppc(posterior_predictive, kind='kde', data_pairs={'y_obs': 'y_obs'})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851b878-4927-4124-885f-d98fb9ac824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.plot_forest(idata, var_names=[\"beta\"], combined=True, hdi_prob=0.95, r_hat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1cd4b-d1f4-4867-955b-5df97334bf18",
   "metadata": {},
   "source": [
    "#### Trace plots\n",
    "\n",
    "Interpretation of Trace Plots\n",
    "\n",
    "    Density Plots (Left Column):\n",
    "        Each subplot on the left shows the kernel density estimate of the posterior distribution for a parameter.\n",
    "        These plots give an idea of the central tendency (mean or median) and the spread (variance) of the parameter estimates.\n",
    "        For example, the density plot for beta shows multiple colored curves corresponding to different chains, indicating the posterior distributions of the coefficients.\n",
    "\n",
    "    Trace Plots (Right Column):\n",
    "        Each subplot on the right shows the sampled values of the parameter across iterations for each chain.\n",
    "        These plots help in assessing the convergence of the Markov Chain Monte Carlo (MCMC) sampling.\n",
    "        A good trace plot should look like a \"hairy caterpillar,\" with the chains mixing well and no apparent trends or patterns over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6da08-c651-48d7-93c9-ca8410413560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the trace plot\n",
    "trace_plot = az.plot_trace(idata)\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('temp_files/report/9. trace_plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101f5ba-a698-4133-a752-2605f5cf4f2b",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d93af3-4191-48dc-9930-bf49dd2637a7",
   "metadata": {},
   "source": [
    "#### Spatial Residual Plots \n",
    "\n",
    "Plot residuals (the differences between observed and predicted values) over the spatial domain to check for patterns. Randomly distributed residuals indicate a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1adc2-cb59-43bf-9700-1ad7de8bd4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed data\n",
    "y_obs = posterior_predictive.observed_data['y_obs'].values\n",
    "\n",
    "# Calculate the mean of the simulated data across chains and draws\n",
    "y_sim = posterior_predictive.posterior_predictive['y_obs'].mean(dim=('chain', 'draw')).values\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_obs - y_sim\n",
    "\n",
    "# Plot spatial residuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "sc = plt.scatter(coordinates_observed[:, 0], coordinates_observed[:, 1], c=residuals, cmap='coolwarm', s=100)\n",
    "plt.colorbar(sc, label='Residuals')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82876d73-6a69-4288-af0a-7adc94733b35",
   "metadata": {},
   "source": [
    "### Uncertainty Quantification\n",
    "\n",
    "\t•\tCredible Intervals: Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values.\n",
    "\t•\tUncertainty Maps: Generate maps of prediction uncertainty to visualize areas of high and low certainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dd7bc-7f8d-4a21-a3d0-a98b000d7cf8",
   "metadata": {},
   "source": [
    "#### Credible Intervals\n",
    "\n",
    "Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457e3cf-0b27-4744-995e-0a991fe1d6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed and simulated data\n",
    "y_obs = response\n",
    "\n",
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Extract the mean prediction, lower and upper bounds of the 95% credible intervals\n",
    "y_sim = posterior_predictive.posterior_predictive['y_obs'].mean(dim=(\"chain\", \"draw\")).values\n",
    "hdi = az.hdi(posterior_predictive.posterior_predictive, hdi_prob=0.95)['y_obs']\n",
    "\n",
    "# Calculate the width of the credible intervals\n",
    "ci_width = hdi[:, 1] - hdi[:, 0]\n",
    "\n",
    "# Check how many true values are within the credible intervals\n",
    "within_ci = np.sum((y_obs >= hdi[:, 0]) & (y_obs <= hdi[:, 1]))\n",
    "total_obs = len(y_obs)\n",
    "coverage = within_ci / total_obs\n",
    "\n",
    "print(f\"Coverage of 95% Credible Intervals: {coverage * 100:.2f}%\")\n",
    "\n",
    "# Plot the credible intervals vs the observed values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(np.arange(len(y_obs)), y_sim, yerr=[y_sim - hdi[:, 0], hdi[:, 1] - y_sim], fmt='o', label='Predictions with 95% CI')\n",
    "plt.plot(np.arange(len(y_obs)), y_obs, 'r.', label='Observed Values')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Predictions with 95% Credible Intervals vs Observed Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the width of the credible intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ci_width, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Width of 95% Credible Intervals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Width of 95% Credible Intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757b68f-dde4-4c13-af21-553bbf65e211",
   "metadata": {},
   "source": [
    "### Transform the results back to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa25cee-6ed3-4eda-b974-01c6564f4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b01b7-7230-497b-ba1c-67884369f483",
   "metadata": {},
   "source": [
    "#### Revert observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3316f3-7462-4bef-9a2a-4fb266ed43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_denormalized = functions.revert_standardization(response, report['Target mean'], report['Target std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fde956-1231-4817-8815-9943d34b44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_original_scale = functions.revert_transformation(y_denormalized, \n",
    "                                report['Transformation Applied'],\n",
    "                                report['Target mean'], \n",
    "                                report['Target std'], \n",
    "                               report['Lambda']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f3087-4ef5-487c-acb2-cff8cbeb25e0",
   "metadata": {},
   "source": [
    "#### Revert predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acbab5-a3fe-4408-9f8f-181c272be5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sim_dn = functions.revert_standardization(y_sim, report['Target mean'], report['Target std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69f01f-8c6c-42bb-a67c-4e0d06bac966",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sim_original_scale = functions.revert_transformation(y_sim_dn, \n",
    "                                report['Transformation Applied'],\n",
    "                                report['Target mean'], \n",
    "                                report['Target std'], \n",
    "                               report['Lambda']).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b31e396-6a10-4766-8b4c-9bd934c6ed96",
   "metadata": {},
   "source": [
    "### Prediction Accuracy Metrics (at original scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e09b40-8184-48ac-b941-7bab86e785ca",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "Measures the average magnitude of the errors in a set of predictions, without considering their direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ebbcd9-d42f-418a-988d-07097e2db1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute errors\n",
    "absolute_errors = np.abs(y_original_scale - y_sim_original_scale)\n",
    "\n",
    "# Calculate the Mean Absolute Error\n",
    "mae = np.mean(absolute_errors)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5b08d-8cac-488a-a542-d67043237737",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Target mean pre transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a3b8a7-31b3-4388-aa74-d0dc1ed9f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Bayesian model MAE'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45621bab-87c4-4f2f-9eaa-2e06b8f995a0",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE) \n",
    "\n",
    "Measures the square root of the average of squared differences between predicted and observed values, providing an indication of the model’s overall error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80301fa7-627b-48dd-8501-6f5a24a7c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the squared errors\n",
    "squared_errors = np.square(y_original_scale - y_sim_original_scale)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mse = np.mean(squared_errors)\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60126662-399d-4453-86be-254279f8a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "report['Bayesian RMSE'] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68477e3a-d926-4b5c-9048-75bbb9112e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save report as pickle\n",
    "with open('temp_files/report/report.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(report, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19089cb-d79a-4844-8b54-e1bf50c75e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f95e5-c14c-4471-905a-fdcd3ef49d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pymc_env",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
