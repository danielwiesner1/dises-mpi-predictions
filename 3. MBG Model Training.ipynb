{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400cf36e-fd75-4aa5-ab3c-435b61d429f6",
   "metadata": {},
   "source": [
    "# Bayesian MBG estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1a93-6dcf-4650-b132-0804b7ea15bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Geostatistical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b76b10a-d4d2-46e3-82c9-33f84a74183d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "#import aesara.tensor as at\n",
    "#from aesara.graph.basic import Constant\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytensor.tensor as at\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e963da62-f8d4-45f8-9b99-27a5d861889d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPU cores to max out the machine in the traning stage\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfb7c0-e567-4932-a955-80bc250cc348",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7895e6-677b-4691-b151-d288cc4d333e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_indicator = 'mpi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5c6412-da59-4b27-982a-ca71851c01ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the directory where the pickle files are stored\n",
    "pickle_dir = 'temp_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf4655-7c75-4007-a06e-f57d8d3c1106",
   "metadata": {},
   "source": [
    "### Load the target and the covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf6c885-def4-492f-b33e-e637bf2b9ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load coordinates from the pickle file\n",
    "with open(os.path.join(pickle_dir, 'coordinates.pkl'), 'rb') as f:\n",
    "    coordinates = pickle.load(f)\n",
    "\n",
    "# Load coordinates for observed rows from the pickle file\n",
    "with open(os.path.join(pickle_dir, 'coordinates_observed.pkl'), 'rb') as f:\n",
    "    coordinates_observed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee20c5a-cbaa-447c-ada0-e48a2cfa59d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('temp_files/selected_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d6189a-79f0-4873-a516-9146b12464df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = df.columns.to_list()\n",
    "\n",
    "# Remove target_values and others\n",
    "remove_list = [target_indicator, 'geometry', 'grid_id']\n",
    "\n",
    "# Remove elements in remove_list from main_list\n",
    "selected_features = [item for item in selected_features if item not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ad7694-6275-464b-bbea-dddf156a2875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only rows with observed target indicator\n",
    "df1 = df[~df[target_indicator].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f3d5cb-2a48-49c6-b77c-c8bc7634df23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61dee749-e44c-41bd-9673-7361cd837b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariate matrix\n",
    "X = df1[selected_features].values\n",
    "\n",
    "# Series with the target variable observed\n",
    "response = df1[target_indicator].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95225ff3-20cb-425d-b022-a318447b4ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize features and response\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "response = (response - response.mean()) / response.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bee51-f03b-413a-bdc8-1d0450100073",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Key Components of the Model\n",
    "\n",
    "    Priors:\n",
    "        beta: Coefficients for the linear model, assumed to follow a normal distribution with mean 0 and standard deviation 1.\n",
    "        sigma: Standard deviation of the observation noise, assumed to follow a half-normal distribution with standard deviation 1.\n",
    "        ls: Length-scale parameter for the spatial covariance function, assumed to follow a half-Cauchy distribution with scale parameter 1.\n",
    "\n",
    "    Spatial Distance Matrix:\n",
    "        D: Matrix of Euclidean distances between all pairs of observed locations.\n",
    "\n",
    "    Covariance Function:\n",
    "        K: Covariance function (Matern 5/2) which defines the spatial correlation structure.\n",
    "\n",
    "    Gaussian Process (GP):\n",
    "        gp: Latent Gaussian process with the defined covariance function.\n",
    "        f: Prior distribution of the GP evaluated at the observed coordinates.\n",
    "\n",
    "    Linear Model:\n",
    "        mu: Mean of the linear model which is a combination of the linear predictor (X * beta) and the spatial effect (f).\n",
    "        y_obs: Observed responses, modeled as a normal distribution with mean mu and standard deviation sigma.\n",
    "\n",
    "    Inference:\n",
    "        Using Automatic Differentiation Variational Inference (ADVI) to approximate the posterior distribution of the model parameters.\n",
    "        advi_fit: Fitting the model using ADVI.\n",
    "        trace: Sampling from the fitted model to obtain posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa606405-d245-4987-a9e2-837650ae2bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors run\n",
      "Distance matrix calculated\n",
      "Covariance run\n",
      "Linear model specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (16 chains in 16 jobs)\n",
      "NUTS: [beta, sigma, ls, f_rotated_]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1b4434ce5141c3b02c3a54ba169050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit a Bayesian geostatistical model\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    ls = pm.HalfCauchy('ls', beta=1)\n",
    "\n",
    "    print('Priors run')\n",
    "\n",
    "    # Spatial distance matrix\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    print('Distance matrix calculated')\n",
    "\n",
    "    # Define covariance function\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    print('Covariance run')\n",
    "\n",
    "    # Linear model\n",
    "    ## This defines the mean of the normal distribution for the observed data. It combines a linear regression term (pm.math.dot(X, beta)) with the GP latent function f.\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "\n",
    "    ## This defines the likelihood of the observed data (response) as a normal distribution with mean mu and standard deviation sigma.\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=response)\n",
    "\n",
    "    print('Linear model specified')\n",
    "\n",
    "    # Inference\n",
    "    step = pm.NUTS(target_accept=0.95)\n",
    "    idata = pm.sample(1000, tune=1000, step=step, cores=num_cores, return_inferencedata=True) #The num_cores parameter maxes the machine out. Tweak if needed. \n",
    "\n",
    "    print('Model Fitted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b14e9-5098-4917-9afd-8fe2ebc7ffa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This bit is to save the trained model in case is needed\n",
    "# Save the InferenceData (trace)\n",
    "trace_filename = 'model_trace.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f73a49-d761-45dd-8455-09767a68fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.to_netcdf(idata, trace_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007bc75-b92e-492a-bd8b-e4e67a31b734",
   "metadata": {},
   "source": [
    "### Reload the model\n",
    "- In case of crash load this insted of the fitting cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7141063-436e-4748-9423-67075141ed99",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Load the model\n",
    "idata = az.from_netcdf(trace_filename)\n",
    "\n",
    "# Re-create the model\n",
    "with pm.Model() as model:\n",
    "    # Re-create the model's priors and components\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    ls = pm.HalfCauchy('ls', beta=1)\n",
    "\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b3166-cca2-409b-8bcc-ee678ac1673f",
   "metadata": {},
   "source": [
    "## Testing the model results\n",
    "\n",
    "1. Posterior Predictive Checks\n",
    "2. Prediction Accuracy Metrics\n",
    "3. Residual Analysis\n",
    "4. Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d960452-f2b8-480e-8571-17dd1e06afe0",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks:\n",
    "\n",
    "Posterior Predictive Distribution: Compare the observed data to the posterior predictive distribution of the model. This involves generating new data based on the posterior distributions of the model parameters and comparing these simulated data to the actual observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3707ef-231e-443a-9bb1-13368730d3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Plot posterior predictive checks\n",
    "az.plot_ppc(posterior_predictive, kind='kde', data_pairs={'y_obs': 'y_obs'})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851b878-4927-4124-885f-d98fb9ac824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.plot_forest(idata, var_names=[\"beta\"], combined=True, hdi_prob=0.95, r_hat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d434ce-055c-4c24-8f3d-8bd0c785e6db",
   "metadata": {},
   "source": [
    "### Prediction Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b035d-f784-437e-8f40-a14dcb197c73",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "Measures the average magnitude of the errors in a set of predictions, without considering their direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ad677-5765-4b4a-a1bf-ea8180ab7891",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE) \n",
    "\n",
    "Measures the square root of the average of squared differences between predicted and observed values, providing an indication of the model’s overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b2c0e-ed1b-4332-98b2-dd63646a8624",
   "metadata": {},
   "source": [
    "#### Coverage Probability\n",
    "\n",
    "The proportion of observed data points that lie within a specified credible interval (e.g., 95% credible interval) of the predicted distribution. High coverage indicates that the model’s uncertainty estimates are reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101f5ba-a698-4133-a752-2605f5cf4f2b",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d93af3-4191-48dc-9930-bf49dd2637a7",
   "metadata": {},
   "source": [
    "#### Spatial Residual Plots \n",
    "\n",
    "Plot residuals (the differences between observed and predicted values) over the spatial domain to check for patterns. Randomly distributed residuals indicate a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25f1adc2-cb59-43bf-9700-1ad7de8bd4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posterior_predictive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract the observed and simulated data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m y_obs \u001b[38;5;241m=\u001b[39m response\n\u001b[0;32m----> 3\u001b[0m y_sim \u001b[38;5;241m=\u001b[39m \u001b[43mposterior_predictive\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_obs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Mean prediction for each observed data point\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate residuals\u001b[39;00m\n\u001b[1;32m      6\u001b[0m residuals \u001b[38;5;241m=\u001b[39m y_obs \u001b[38;5;241m-\u001b[39m y_sim\n",
      "\u001b[0;31mNameError\u001b[0m: name 'posterior_predictive' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the observed and simulated data\n",
    "y_obs = response\n",
    "y_sim = posterior_predictive['y_obs'].mean(axis=0)  # Mean prediction for each observed data point\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_obs - y_sim\n",
    "\n",
    "# Plot spatial residuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "sc = plt.scatter(coordinates_observed[:, 0], coordinates_observed[:, 1], c=residuals, cmap='coolwarm', s=100)\n",
    "plt.colorbar(sc, label='Residuals')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82876d73-6a69-4288-af0a-7adc94733b35",
   "metadata": {},
   "source": [
    "### Uncertainty Quantification\n",
    "\n",
    "\t•\tCredible Intervals: Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values.\n",
    "\t•\tUncertainty Maps: Generate maps of prediction uncertainty to visualize areas of high and low certainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dd7bc-7f8d-4a21-a3d0-a98b000d7cf8",
   "metadata": {},
   "source": [
    "#### Credible Intervals\n",
    "\n",
    "Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457e3cf-0b27-4744-995e-0a991fe1d6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed and simulated data\n",
    "y_obs = response\n",
    "\n",
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Extract the mean prediction, lower and upper bounds of the 95% credible intervals\n",
    "y_sim = posterior_predictive.posterior_predictive['y_obs'].mean(dim=(\"chain\", \"draw\")).values\n",
    "hdi = az.hdi(posterior_predictive.posterior_predictive, hdi_prob=0.95)['y_obs']\n",
    "\n",
    "# Calculate the width of the credible intervals\n",
    "ci_width = hdi[:, 1] - hdi[:, 0]\n",
    "\n",
    "# Check how many true values are within the credible intervals\n",
    "within_ci = np.sum((y_obs >= hdi[:, 0]) & (y_obs <= hdi[:, 1]))\n",
    "total_obs = len(y_obs)\n",
    "coverage = within_ci / total_obs\n",
    "\n",
    "print(f\"Coverage of 95% Credible Intervals: {coverage * 100:.2f}%\")\n",
    "\n",
    "# Plot the credible intervals vs the observed values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(np.arange(len(y_obs)), y_sim, yerr=[y_sim - hdi[:, 0], hdi[:, 1] - y_sim], fmt='o', label='Predictions with 95% CI')\n",
    "plt.plot(np.arange(len(y_obs)), y_obs, 'r.', label='Observed Values')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Predictions with 95% Credible Intervals vs Observed Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the width of the credible intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ci_width, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Width of 95% Credible Intervals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Width of 95% Credible Intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ea41e-c871-4dc0-b9c8-cdc082e71b35",
   "metadata": {},
   "source": [
    "#### Trace plots\n",
    "\n",
    "Interpretation of Trace Plots\n",
    "\n",
    "    Density Plots (Left Column):\n",
    "        Each subplot on the left shows the kernel density estimate of the posterior distribution for a parameter.\n",
    "        These plots give an idea of the central tendency (mean or median) and the spread (variance) of the parameter estimates.\n",
    "        For example, the density plot for beta shows multiple colored curves corresponding to different chains, indicating the posterior distributions of the coefficients.\n",
    "\n",
    "    Trace Plots (Right Column):\n",
    "        Each subplot on the right shows the sampled values of the parameter across iterations for each chain.\n",
    "        These plots help in assessing the convergence of the Markov Chain Monte Carlo (MCMC) sampling.\n",
    "        A good trace plot should look like a \"hairy caterpillar,\" with the chains mixing well and no apparent trends or patterns over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6da08-c651-48d7-93c9-ca8410413560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the trace plot\n",
    "trace_plot = az.plot_trace(idata)\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('temp_files/report/9. trace_plot.pdf')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pymc_env",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
