{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400cf36e-fd75-4aa5-ab3c-435b61d429f6",
   "metadata": {},
   "source": [
    "# Bayesian MBG estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894443c2-6f53-4876-87a5-0ae9196380b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load preselected variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb178a26-2b34-465f-a495-ecd88d907887",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Geostatistical Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6cd79a-32fd-4894-8dd7-c583e8a11e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "#import aesara.tensor as at\n",
    "#from aesara.graph.basic import Constant\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pytensor.tensor as at\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db736d23-3a14-48bb-8ffd-fa4728437a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 8\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPU cores to max out the machine in the traning stage\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9b3df-09bf-413b-83ef-368d2a7019a1",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d27d47-a33a-47c7-a255-1a4627a222cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_indicator = 'mpi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714abcb5-6d9d-405a-b0ab-149969c1e1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the directory where the pickle files are stored\n",
    "pickle_dir = 'temp_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602103ef-4dff-4e36-b9c9-c097f6d15b57",
   "metadata": {},
   "source": [
    "### Load the target and the covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c47430-8e45-4082-bf9c-328e25a9f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load coordinates from the pickle file\n",
    "with open(os.path.join(pickle_dir, 'coordinates.pkl'), 'rb') as f:\n",
    "    coordinates = pickle.load(f)\n",
    "\n",
    "# Load coordinates for observed rows from the pickle file\n",
    "with open(os.path.join(pickle_dir, 'coordinates_observed.pkl'), 'rb') as f:\n",
    "    coordinates_observed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409f213f-8414-4f03-8be9-2afee28fdbf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('temp_files/selected_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea1c0f3-3d32-47b2-81f8-02c388f71d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = df.columns.to_list()\n",
    "\n",
    "# Remove target_values and others\n",
    "remove_list = [target_indicator, 'geometry', 'grid_id']\n",
    "\n",
    "# Remove elements in remove_list from main_list\n",
    "selected_features = [item for item in selected_features if item not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8129a75e-3878-47ef-8df9-7d83c3cfd474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Only rows with observed target indicator\n",
    "df1 = df[~df[target_indicator].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91b2200-c0db-4daf-b104-5d14cfc09d16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d70c106-0aab-4bb8-9166-d16cf1efff9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Covariate matrix\n",
    "X = df1[selected_features].values\n",
    "\n",
    "# Series with the target variable observed\n",
    "response = df1[target_indicator].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0be0385-869b-4dc9-9bb0-400007dbd2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize features and response\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "response = (response - response.mean()) / response.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cdc32f-0a93-400f-bd52-1fe057338784",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Key Components of the Model\n",
    "\n",
    "    Priors:\n",
    "        beta: Coefficients for the linear model, assumed to follow a normal distribution with mean 0 and standard deviation 1.\n",
    "        sigma: Standard deviation of the observation noise, assumed to follow a half-normal distribution with standard deviation 1.\n",
    "        ls: Length-scale parameter for the spatial covariance function, assumed to follow a half-Cauchy distribution with scale parameter 1.\n",
    "\n",
    "    Spatial Distance Matrix:\n",
    "        D: Matrix of Euclidean distances between all pairs of observed locations.\n",
    "\n",
    "    Covariance Function:\n",
    "        K: Covariance function (Matern 5/2) which defines the spatial correlation structure.\n",
    "\n",
    "    Gaussian Process (GP):\n",
    "        gp: Latent Gaussian process with the defined covariance function.\n",
    "        f: Prior distribution of the GP evaluated at the observed coordinates.\n",
    "\n",
    "    Linear Model:\n",
    "        mu: Mean of the linear model which is a combination of the linear predictor (X * beta) and the spatial effect (f).\n",
    "        y_obs: Observed responses, modeled as a normal distribution with mean mu and standard deviation sigma.\n",
    "\n",
    "    Inference:\n",
    "        Using Automatic Differentiation Variational Inference (ADVI) to approximate the posterior distribution of the model parameters.\n",
    "        advi_fit: Fitting the model using ADVI.\n",
    "        trace: Sampling from the fitted model to obtain posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3473f92-d119-4434-b101-d2d4ff458041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors run\n",
      "Distance matrix calculated\n",
      "Covariance run\n",
      "Linear model specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (8 chains in 8 jobs)\n",
      "NUTS: [beta, sigma, ls, f_rotated_]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618ad590b29d48d397c48f0aa52f5564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 8 chains for 1_000 tune and 1_000 draw iterations (8_000 + 8_000 draws total) took 331 seconds.\n",
      "There were 266 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Fitted\n",
      "CPU times: user 22.8 s, sys: 1.95 s, total: 24.7 s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit a Bayesian geostatistical model\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, shape=len(selected_features))\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    ls = pm.HalfCauchy('ls', beta=1)\n",
    "\n",
    "    print('Priors run')\n",
    "\n",
    "    # Spatial distance matrix\n",
    "    D = np.sqrt(((coordinates_observed[:, None, :] - coordinates_observed[None, :, :])**2).sum(axis=-1))\n",
    "\n",
    "    print('Distance matrix calculated')\n",
    "\n",
    "    # Define covariance function\n",
    "    K = pm.gp.cov.Matern52(2, ls=ls)\n",
    "    gp = pm.gp.Latent(cov_func=K)\n",
    "    f = gp.prior('f', X=coordinates_observed)\n",
    "\n",
    "    print('Covariance run')\n",
    "\n",
    "    # Linear model\n",
    "    ## This defines the mean of the normal distribution for the observed data. It combines a linear regression term (pm.math.dot(X, beta)) with the GP latent function f.\n",
    "    mu = pm.math.dot(X, beta) + f\n",
    "\n",
    "    ## This defines the likelihood of the observed data (response) as a normal distribution with mean mu and standard deviation sigma.\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=response)\n",
    "\n",
    "    print('Linear model specified')\n",
    "\n",
    "    # Inference\n",
    "    step = pm.NUTS(target_accept=0.95)\n",
    "    idata = pm.sample(1000, tune=1000, step=step, cores=num_cores, return_inferencedata=True) #The num_cores parameter maxes the machine out. Tweak if needed. \n",
    "\n",
    "    print('Model Fitted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f73a49-d761-45dd-8455-09767a68fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This bit is to save the trained model in case is needed\n",
    "# Save the InferenceData (trace)\n",
    "#trace_filename = 'model_trace.nc'\n",
    "\n",
    "#az.to_netcdf(idata, trace_filename)\n",
    "# Load the trace\n",
    "#trace_filename = 'model_trace.nc'\n",
    "#loaded_trace = az.from_netcdf(trace_filename)\n",
    "\n",
    "#print(\"Trace successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c0504-c686-496f-90fc-8810d74a483a",
   "metadata": {},
   "source": [
    "## Generating predictions for new grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec5b830-547e-43a4-aa16-8b6cbfc3425a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out rows with NaN values in target indicator\n",
    "df2 = df[df[target_indicator].isnull()] # Only unobserved rows\n",
    "\n",
    "# Covariate matrix\n",
    "X_new = df2[selected_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e67f4a4-c412-4528-bc9d-262380d785bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize the new data using the same scaler fitted on the observed data\n",
    "X_new = (X_new - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Load coordinates for the new locations\n",
    "with open(os.path.join(pickle_dir, 'coordinates_unobserved.pkl'), 'rb') as f:\n",
    "    coordinates_new = pickle.load(f)\n",
    "\n",
    "# Ensure the scaler is fitted on the same data\n",
    "scaler = StandardScaler().fit(df1[selected_features].values)\n",
    "X_new = scaler.transform(df2[selected_features].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83bd58-f900-4554-875a-9bc99ec6f17b",
   "metadata": {},
   "source": [
    "### Check if the covariance matrix is PSD\n",
    "- PSD: Positive Semi-Definitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7ae0015-27f1-47b4-9cce-fd6406b70533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor.tensor as at\n",
    "\n",
    "#This function is key to diagnose what is going on inside the Gaussian process\n",
    "\n",
    "def diagnose_covariance_matrix(cov, jitter=1e-6):\n",
    "    \"\"\"\n",
    "    Diagnose potential issues with a covariance matrix and suggest possible remedies.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    cov : np.ndarray or pytensor.tensor\n",
    "        The covariance matrix to diagnose.\n",
    "    jitter : float, optional\n",
    "        The amount of jitter to add to the diagonal of the covariance matrix for stabilization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert pytensor tensor to numpy array for diagnosis if necessary\n",
    "    if isinstance(cov, at.TensorVariable):\n",
    "        # Use pm.draw to evaluate the tensor as a NumPy array\n",
    "        cov = pm.draw(cov)\n",
    "\n",
    "    # Check for symmetry\n",
    "    if not np.allclose(cov, cov.T):\n",
    "        print(\"Warning: Covariance matrix is not symmetric.\")\n",
    "    else:\n",
    "        print(\"Covariance matrix is symmetric.\")\n",
    "\n",
    "    # Check for positive semi-definiteness using eigenvalues\n",
    "    eigvals = np.linalg.eigvalsh(cov)\n",
    "    if np.all(eigvals >= 0):\n",
    "        print(\"Covariance matrix is positive semi-definite (PSD).\")\n",
    "    elif np.all(eigvals > 0):\n",
    "        print(\"Covariance matrix is positive definite (PD).\")\n",
    "    else:\n",
    "        print(\"Covariance matrix is not positive semi-definite (non-PSD).\")\n",
    "        print(\"Eigenvalues:\")\n",
    "        print(eigvals)\n",
    "\n",
    "    # Check for small or negative eigenvalues\n",
    "    if np.any(eigvals < 0):\n",
    "        print(\"There are negative eigenvalues, indicating non-PSD matrix.\")\n",
    "    elif np.any(eigvals == 0):\n",
    "        print(\"There are zero eigenvalues, indicating the matrix is singular or nearly singular.\")\n",
    "    if np.any(eigvals < jitter):\n",
    "        print(\"Some eigenvalues are smaller than the jitter value. Consider increasing jitter.\")\n",
    "\n",
    "    # Check the condition number (ratio of max to min eigenvalue)\n",
    "    cond_number = np.linalg.cond(cov)\n",
    "    print(f\"Condition number of the matrix: {cond_number:.2e}\")\n",
    "    if cond_number > 1e10:\n",
    "        print(\"Warning: Covariance matrix is ill-conditioned (large condition number).\")\n",
    "        print(\"Consider regularization or using a different covariance function.\")\n",
    "\n",
    "    # Suggest adding jitter and re-check PSD\n",
    "    cov_with_jitter = cov + jitter * np.eye(cov.shape[0])\n",
    "    eigvals_with_jitter = np.linalg.eigvalsh(cov_with_jitter)\n",
    "    if np.all(eigvals_with_jitter >= 0):\n",
    "        print(\"Adding jitter made the covariance matrix positive semi-definite.\")\n",
    "    else:\n",
    "        print(\"Even after adding jitter, the matrix is still not positive semi-definite.\")\n",
    "\n",
    "    # Check for numerical issues using Cholesky decomposition\n",
    "    try:\n",
    "        np.linalg.cholesky(cov)\n",
    "        print(\"Cholesky decomposition succeeded: Covariance matrix is positive definite.\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Cholesky decomposition failed: Covariance matrix is not positive definite.\")\n",
    "\n",
    "    print(\"\\nDiagnosis Complete.\")\n",
    "\n",
    "def generate_predictions(model, coordinates_new, X_new, idata, initial_jitter=1e-6, max_attempts=5):\n",
    "    \"\"\"\n",
    "    Generate predictions for new data using a Gaussian Process model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : pm.Model\n",
    "        The PyMC model object that contains the Gaussian Process.\n",
    "    coordinates_new : np.ndarray\n",
    "        An array of coordinates for the new data points where predictions are needed.\n",
    "    X_new : np.ndarray\n",
    "        The covariate matrix for the new data points.\n",
    "    idata : az.InferenceData\n",
    "        The InferenceData object containing posterior samples from the fitted model.\n",
    "    initial_jitter : float, optional\n",
    "        The initial jitter value to add to the covariance matrix to ensure positive definiteness.\n",
    "    max_attempts : int, optional\n",
    "        Maximum number of attempts to find a stable jitter value.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of mean predictions for the new data points.\n",
    "    \"\"\"\n",
    "\n",
    "    with model:\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                jitter = initial_jitter * (10 ** attempt)\n",
    "                unique_name = \"f_pred_\" + str(uuid.uuid4())\n",
    "\n",
    "                # Generate the conditional GP with added jitter to the covariance matrix\n",
    "                f_pred = gp.conditional(unique_name, coordinates_new, jitter=jitter)\n",
    "\n",
    "                # Compute the mean of the beta samples\n",
    "                beta_mean = idata.posterior['beta'].mean(dim=(\"chain\", \"draw\")).values\n",
    "\n",
    "                # Predictive mean\n",
    "                mu_pred = pm.math.dot(X_new, beta_mean) + f_pred\n",
    "\n",
    "                # Create the covariance matrix using PyMC's Matern32\n",
    "                cov = pm.gp.cov.Matern32(coordinates_new.shape[1], ls=1.0)(coordinates_new)\n",
    "\n",
    "                # Add jitter using Pytensor's identity matrix\n",
    "                cov += jitter * at.eye(cov.shape[0])\n",
    "\n",
    "                # Symmetrize the covariance matrix to ensure symmetry\n",
    "                cov = (cov + cov.T) / 2\n",
    "\n",
    "                # Check cov_matrix before predictions\n",
    "                diagnose_covariance_matrix(cov)\n",
    "                \n",
    "                # Check for positive definiteness using Cholesky decomposition\n",
    "                _ = at.slinalg.cholesky(cov)\n",
    "\n",
    "                # If successful, proceed with prediction\n",
    "                pred_samples = pm.sample_posterior_predictive(idata, var_names=[unique_name], return_inferencedata=True)\n",
    "                return pred_samples.posterior_predictive[unique_name].mean(axis=0)\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == max_attempts - 1:\n",
    "                    raise ValueError(f\"The covariance matrix is not positive semi-definite even after {max_attempts} attempts with increasing jitter.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1cc640-e164-461e-8172-9d4bdbf6a695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 1.16e+07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_6b367b46-6a07-4780-9296-e2bed9c93d87]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aded0d130928464a9a5513a0edeb26a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 9.45e+06\n",
      "Adding jitter made the covariance matrix positive semi-definite.\n",
      "Cholesky decomposition succeeded: Covariance matrix is positive definite.\n",
      "\n",
      "Diagnosis Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_f049a6a4-d3b2-4ec7-89ca-2041568fe2ad]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8b71d9443c46529c0e15b9ae9ff190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix is symmetric.\n",
      "Covariance matrix is positive semi-definite (PSD).\n",
      "Condition number of the matrix: 3.32e+06\n",
      "Adding jitter made the covariance matrix positive semi-definite.\n",
      "Cholesky decomposition succeeded: Covariance matrix is positive definite.\n",
      "\n",
      "Diagnosis Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [f_pred_b352282d-1c5a-4c93-ac14-6b0c96c69866]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5733867d838409fb0cf1412e59e0e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "mu_pred = generate_predictions(model, coordinates_new, X_new, idata)\n",
    "\n",
    "# Transform the predictions back to the original scale\n",
    "predicted_response = (mu_pred * response.std()) + response.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d8817-5815-46a5-8b01-b7d95acdbc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_response = pd.DataFrame(predicted_response.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcee860-af0d-4ca4-b8b3-e58a26368efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the results\n",
    "df3 = pd.concat([df2['grid_id'].reset_index(drop=True), predicted_response], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b3166-cca2-409b-8bcc-ee678ac1673f",
   "metadata": {},
   "source": [
    "## Testing the model results\n",
    "\n",
    "1. Posterior Predictive Checks\n",
    "2. Prediction Accuracy Metrics\n",
    "3. Residual Analysis\n",
    "4. Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d960452-f2b8-480e-8571-17dd1e06afe0",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks:\n",
    "\n",
    "Posterior Predictive Distribution: Compare the observed data to the posterior predictive distribution of the model. This involves generating new data based on the posterior distributions of the model parameters and comparing these simulated data to the actual observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3707ef-231e-443a-9bb1-13368730d3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Plot posterior predictive checks\n",
    "az.plot_ppc(posterior_predictive, kind='kde', data_pairs={'y_obs': 'y_obs'})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851b878-4927-4124-885f-d98fb9ac824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "az.plot_forest(idata, var_names=[\"beta\"], combined=True, hdi_prob=0.95, r_hat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d434ce-055c-4c24-8f3d-8bd0c785e6db",
   "metadata": {},
   "source": [
    "### Prediction Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b035d-f784-437e-8f40-a14dcb197c73",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "Measures the average magnitude of the errors in a set of predictions, without considering their direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ad677-5765-4b4a-a1bf-ea8180ab7891",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE) \n",
    "\n",
    "Measures the square root of the average of squared differences between predicted and observed values, providing an indication of the model’s overall error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b2c0e-ed1b-4332-98b2-dd63646a8624",
   "metadata": {},
   "source": [
    "#### Coverage Probability\n",
    "\n",
    "The proportion of observed data points that lie within a specified credible interval (e.g., 95% credible interval) of the predicted distribution. High coverage indicates that the model’s uncertainty estimates are reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101f5ba-a698-4133-a752-2605f5cf4f2b",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d93af3-4191-48dc-9930-bf49dd2637a7",
   "metadata": {},
   "source": [
    "#### Spatial Residual Plots \n",
    "\n",
    "Plot residuals (the differences between observed and predicted values) over the spatial domain to check for patterns. Randomly distributed residuals indicate a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1adc2-cb59-43bf-9700-1ad7de8bd4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed and simulated data\n",
    "y_obs = response\n",
    "y_sim = posterior_predictive['y_obs'].mean(axis=0)  # Mean prediction for each observed data point\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_obs - y_sim\n",
    "\n",
    "# Plot spatial residuals\n",
    "plt.figure(figsize=(12, 8))\n",
    "sc = plt.scatter(coordinates_observed[:, 0], coordinates_observed[:, 1], c=residuals, cmap='coolwarm', s=100)\n",
    "plt.colorbar(sc, label='Residuals')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Spatial Residual Plot')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82876d73-6a69-4288-af0a-7adc94733b35",
   "metadata": {},
   "source": [
    "### Uncertainty Quantification\n",
    "\n",
    "\t•\tCredible Intervals: Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values.\n",
    "\t•\tUncertainty Maps: Generate maps of prediction uncertainty to visualize areas of high and low certainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dd7bc-7f8d-4a21-a3d0-a98b000d7cf8",
   "metadata": {},
   "source": [
    "#### Credible Intervals\n",
    "\n",
    "Evaluate the width of the credible intervals for predictions. Narrower intervals indicate higher precision, but they should still encompass the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457e3cf-0b27-4744-995e-0a991fe1d6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the observed and simulated data\n",
    "y_obs = response\n",
    "\n",
    "# Generate posterior predictive samples for checks\n",
    "with model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(idata, var_names=['y_obs'], return_inferencedata=True)\n",
    "\n",
    "# Extract the mean prediction, lower and upper bounds of the 95% credible intervals\n",
    "y_sim = posterior_predictive.posterior_predictive['y_obs'].mean(dim=(\"chain\", \"draw\")).values\n",
    "hdi = az.hdi(posterior_predictive.posterior_predictive, hdi_prob=0.95)['y_obs']\n",
    "\n",
    "# Calculate the width of the credible intervals\n",
    "ci_width = hdi[:, 1] - hdi[:, 0]\n",
    "\n",
    "# Check how many true values are within the credible intervals\n",
    "within_ci = np.sum((y_obs >= hdi[:, 0]) & (y_obs <= hdi[:, 1]))\n",
    "total_obs = len(y_obs)\n",
    "coverage = within_ci / total_obs\n",
    "\n",
    "print(f\"Coverage of 95% Credible Intervals: {coverage * 100:.2f}%\")\n",
    "\n",
    "# Plot the credible intervals vs the observed values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(np.arange(len(y_obs)), y_sim, yerr=[y_sim - hdi[:, 0], hdi[:, 1] - y_sim], fmt='o', label='Predictions with 95% CI')\n",
    "plt.plot(np.arange(len(y_obs)), y_obs, 'r.', label='Observed Values')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Predictions with 95% Credible Intervals vs Observed Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the width of the credible intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ci_width, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Width of 95% Credible Intervals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Width of 95% Credible Intervals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa99f0-8955-41cb-b75f-c168089f50cf",
   "metadata": {},
   "source": [
    "#### Credible intervals for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27cb17-1696-4e16-8b8a-7527e50eeaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate predictions using posterior samples\n",
    "def generate_predictions(model, coordinates_new, X_new, idata):\n",
    "    with model:\n",
    "        jitter = 1e-5  # Increase the jitter value\n",
    "        f_pred = gp.conditional('f_pred', coordinates_new, jitter=jitter)\n",
    "        beta_mean = idata.posterior['beta'].mean(dim=(\"chain\", \"draw\")).values\n",
    "        mu_pred = pm.math.dot(X_new, beta_mean) + f_pred\n",
    "        pred_samples = pm.sample_posterior_predictive(idata, var_names=[\"f_pred\"], return_inferencedata=True)\n",
    "        return mu_pred, pred_samples\n",
    "\n",
    "# Generate predictions\n",
    "mu_pred, pred_samples = generate_predictions(model, coordinates_new, X_new, idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dda18e-fba5-4b56-bd56-cc73685f2466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the samples for the predicted function values\n",
    "f_pred_samples = pred_samples.posterior_predictive[\"f_pred\"]\n",
    "\n",
    "# Compute the 95% credible intervals\n",
    "hdi = az.hdi(f_pred_samples, hdi_prob=0.95)\n",
    "\n",
    "# Extract mean predictions\n",
    "mu_pred_mean = f_pred_samples.mean(dim=(\"chain\", \"draw\")).values\n",
    "\n",
    "# Extract lower and upper bounds of the credible intervals\n",
    "ci_lower = hdi.sel(hdi=\"lower\").to_array().values.flatten()\n",
    "ci_upper = hdi.sel(hdi=\"higher\").to_array().values.flatten()\n",
    "\n",
    "# Calculate the width of the credible intervals\n",
    "ci_width = ci_upper - ci_lower\n",
    "\n",
    "# Plot the credible intervals for new data\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(np.arange(len(mu_pred_mean)), mu_pred_mean, yerr=[mu_pred_mean - ci_lower, ci_upper - mu_pred_mean], fmt='o', label='Predictions with 95% CI')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predictions with 95% Credible Intervals for New Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the width of the credible intervals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(ci_width, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Width of 95% Credible Intervals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the Width of 95% Credible Intervals for New Data')\n",
    "plt.show()\n",
    "\n",
    "# Generate uncertainty maps\n",
    "plt.figure(figsize=(12, 8))\n",
    "sc = plt.scatter(coordinates_new[:, 0], coordinates_new[:, 1], c=ci_width, cmap='coolwarm', s=100)\n",
    "plt.colorbar(sc, label='Credible Interval Width')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Prediction Uncertainty Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0247740-c22f-4a17-9ee6-ede4f074a325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ci_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf38520-8ad6-4d18-9070-68289bfb30df",
   "metadata": {},
   "source": [
    "#### Uncertainty Maps \n",
    "\n",
    "Generate maps of prediction uncertainty to visualize areas of high and low certainty in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ea41e-c871-4dc0-b9c8-cdc082e71b35",
   "metadata": {},
   "source": [
    "#### Trace plots\n",
    "\n",
    "Interpretation of Trace Plots\n",
    "\n",
    "    Density Plots (Left Column):\n",
    "        Each subplot on the left shows the kernel density estimate of the posterior distribution for a parameter.\n",
    "        These plots give an idea of the central tendency (mean or median) and the spread (variance) of the parameter estimates.\n",
    "        For example, the density plot for beta shows multiple colored curves corresponding to different chains, indicating the posterior distributions of the coefficients.\n",
    "\n",
    "    Trace Plots (Right Column):\n",
    "        Each subplot on the right shows the sampled values of the parameter across iterations for each chain.\n",
    "        These plots help in assessing the convergence of the Markov Chain Monte Carlo (MCMC) sampling.\n",
    "        A good trace plot should look like a \"hairy caterpillar,\" with the chains mixing well and no apparent trends or patterns over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6da08-c651-48d7-93c9-ca8410413560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the trace plot\n",
    "trace_plot = az.plot_trace(idata)\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('temp_files/trace_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3a702-f0f2-4e2d-902e-8a98f1acaad3",
   "metadata": {},
   "source": [
    "### Create modeled surface maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb2629-393e-49e2-96f1-f470f469f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming new_coordinates is a grid, reshape mean predictions\n",
    "x_new = coordinates_new[:, 0].reshape(grid_shape)  # Define grid_shape as per your coordinate grid\n",
    "y_new = coordinates_new[:, 1].reshape(grid_shape)\n",
    "z_pred_mean = y_pred_mean.reshape(grid_shape)\n",
    "\n",
    "# Plot the mean predictions\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x_new, y_new, z_pred_mean, cmap='viridis')\n",
    "\n",
    "plt.title('Modeled Surface Map')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "ax.set_zlabel('Predicted Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f52ad-ae4b-4d81-9012-9760931b859d",
   "metadata": {},
   "source": [
    "### Create uncertainty maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3789a-9004-4f29-ba34-3cb5bf44cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape standard deviation predictions\n",
    "z_pred_std = y_pred_std.reshape(grid_shape)\n",
    "\n",
    "# Plot the uncertainty (standard deviation)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x_new, y_new, z_pred_std, cmap='viridis')\n",
    "\n",
    "plt.title('Uncertainty Map')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "ax.set_zlabel('Standard Deviation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18b168-602a-404a-a501-763af254ba00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pymc_env",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
